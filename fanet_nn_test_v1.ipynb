{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test FANET NN\n",
    "Date: 07/03/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 17:24:12.015157: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-20 17:24:12.126266: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-20 17:24:12.130814: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-07-20 17:24:12.130827: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-20 17:24:12.153427: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-20 17:24:12.694156: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-20 17:24:12.694202: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-20 17:24:12.694207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd # for data manipulation \n",
    "import numpy as np\n",
    "import glob, math\n",
    "from scipy import special\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def euclidean_dist(row):\n",
    "    # Function to calc euclidean distance on every df row \n",
    "    euc_dist = math.sqrt(row[\"U2G_Distance\"]**2 - row[\"Height\"]**2)\n",
    "    return euc_dist\n",
    "\n",
    "def q_func(x):\n",
    "    q = 0.5 - 0.5*special.erf(x / np.sqrt(2))\n",
    "    return q\n",
    "\n",
    "def friis_calc(P,freq,dist,ple):\n",
    "    '''\n",
    "    Friis path loss equation\n",
    "    P = Tx transmit power\n",
    "    freq = Signal frequency\n",
    "    dist = Transmission distance\n",
    "    ple = Path loss exponent\n",
    "    '''\n",
    "    propagation_speed = 299792458\n",
    "    l = propagation_speed / freq\n",
    "    h_pl = P * l**2 / (16*math.pi**2)\n",
    "    P_Rx = h_pl * dist**(-ple)\n",
    "    return P_Rx\n",
    "\n",
    "def plos_calc(h_dist, height_tx, height_rx, env='suburban'):\n",
    "    '''\n",
    "    % This function implements the LoS probability model from the paper\n",
    "    % \"Blockage Modeling for Inter-layer UAVs Communications in Urban\n",
    "    % Environments\" \n",
    "    % param h_dist    : horizontal distance between Tx and Rx (m)\n",
    "    % param height_tx : height of Tx\n",
    "    % param height_rx : height of Rx\n",
    "    '''\n",
    "    if env == 'suburban':\n",
    "        a1 = 0.1\n",
    "        a2 = 7.5e-4\n",
    "        a3 = 8\n",
    "    \n",
    "    delta_h = height_tx - height_rx\n",
    "    # pow_factor = 2 * h_dist * math.sqrt(a1*a2/math.pi) + a1 # NOTE: Use this pow_factor if assuming PPP building dist.\n",
    "    pow_factor = h_dist * math.sqrt(a1*a2) # NOTE: Use this pow_factor if assuming ITU-R assumptions.\n",
    "    if delta_h == 0:\n",
    "        p = (1 - math.exp((-(height_tx)**2) / (2*a3**2))) ** pow_factor\n",
    "    else:\n",
    "        if delta_h < 0:\n",
    "            h1 = height_rx\n",
    "            h2 = height_tx\n",
    "        else:\n",
    "            h1 = height_tx\n",
    "            h2 = height_rx\n",
    "        delta_h = abs(delta_h)\n",
    "        p = (1 - (math.sqrt(2*math.pi)*a3 / delta_h) * abs(q_func(h1/a3) - q_func(h2/a3))) ** pow_factor\n",
    "    return p\n",
    "\n",
    "def sinr_lognormal_approx(h_dist, height, env='suburban'):\n",
    "    '''\n",
    "    To approximate the SNR from signal considering multipath fading and shadowing\n",
    "    Assuming no interference due to CSMA, and fixed noise\n",
    "    Inputs:\n",
    "    h_dist = Horizontal Distance between Tx and Rx\n",
    "    height = Height difference between Tx and Rx\n",
    "    env = The operating environment (currently only suburban supported)\n",
    "    '''\n",
    "    # Signal properties\n",
    "    P_Tx_dBm = 20 # Transmit power of \n",
    "    P_Tx = 10**(P_Tx_dBm/10) / 1000\n",
    "    freq = 2.4e9 # Channel frequency (Hz)\n",
    "    noise_dBm = -86\n",
    "    noise = 10**(noise_dBm/10) / 1000\n",
    "    if env == \"suburban\":\n",
    "        # ENV Parameters Constants ----------------------------------\n",
    "        # n_min = 2\n",
    "        # n_max = 2.75\n",
    "        # K_dB_min = 7.8\n",
    "        # K_dB_max = 17.5\n",
    "        # K_min = 10**(K_dB_min/10)\n",
    "        # K_max = 10**(K_dB_max/10)\n",
    "        # alpha = 11.25 # Env parameters for logarithm std dev of shadowing \n",
    "        # beta = 0.06 # Env parameters for logarithm std dev of shadowing \n",
    "        n_min = 2\n",
    "        n_max = 2.75\n",
    "        K_dB_min = 1.4922\n",
    "        K_dB_max = 12.2272\n",
    "        K_min = 10**(K_dB_min/10)\n",
    "        K_max = 10**(K_dB_max/10)\n",
    "        alpha = 11.1852 # Env parameters for logarithm std dev of shadowing \n",
    "        beta = 0.06 # Env parameters for logarithm std dev of shadowing \n",
    "        # -----------------------------------------------------------\n",
    "    # Calculate fading parameters\n",
    "    PLoS = plos_calc(h_dist, 0, height, env='suburban')\n",
    "    theta_Rx = math.atan2(height, h_dist) * 180 / math.pi # Elevation angle in degrees\n",
    "    ple = (n_min - n_max) * PLoS + n_max # Path loss exponent\n",
    "    sigma_phi_dB = alpha*math.exp(-beta*theta_Rx)\n",
    "    sigma_phi = 10**(sigma_phi_dB/10) # Logarithmic std dev of shadowing\n",
    "    K = K_min * math.exp(math.log(K_max/K_min) * PLoS**2)\n",
    "    omega = 1 # Omega of NCS (Rician)\n",
    "    dist = math.sqrt(h_dist**2 + height**2)\n",
    "    P_Rx = friis_calc(P_Tx, freq, dist, ple)\n",
    "    # Approximate L-NCS RV (which is the SNR) as lognormal\n",
    "    eta = math.log(10) / 10\n",
    "    mu_phi = 10*math.log10(P_Rx)\n",
    "    E_phi = math.exp(eta*mu_phi + eta**2*sigma_phi**2/2) # Mean of shadowing RV\n",
    "    var_phi = math.exp(2*eta*mu_phi+eta**2*sigma_phi**2)*(math.exp(eta**2*sigma_phi**2)-1) # Variance of shadowing RV\n",
    "    E_chi = (special.gamma(1+1)/(1+K))*special.hyp1f1(-1,1,-K)*omega\n",
    "    var_chi = (special.gamma(1+2)/(1+K)**2)*special.hyp1f1(-2,1,-K)*omega**2 - E_chi**2\n",
    "    E_SNR = E_phi * E_chi / noise # Theoretical mean of SINR\n",
    "    var_SNR = ((var_phi+E_phi**2)*(var_chi+E_chi**2) - E_phi**2 * E_chi**2) / noise**2\n",
    "    std_dev_SNR = math.sqrt(var_SNR)\n",
    "    # sigma_ln = math.sqrt(math.log(var_SNR/E_SNR**2 + 1))\n",
    "    # mu_ln = math.log(E_SNR) - sigma_ln**2/2\n",
    "    return E_SNR, std_dev_SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122.743643457063 465.2159856885714\n"
     ]
    }
   ],
   "source": [
    "[m,s] = sinr_lognormal_approx(0,60)\n",
    "print(m,s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NN Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 15:37:11.820016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-02 15:37:11.820138: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-02 15:37:11.820218: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-02 15:37:11.820292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-02 15:37:11.820365: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-02 15:37:11.820439: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-02 15:37:11.820509: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-02 15:37:11.820581: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-05-02 15:37:11.820596: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-02 15:37:11.821031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v2-2_21032023/model.010-2.0131.h5\", compile=False)\n",
    "# model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v2_hovering/model.001-0.9263.h5\", compile=False)\n",
    "# model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v2_hovering_sinr/model.001-0.9614.h5\", compile=False)\n",
    "# model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v3_hovering/model.001-0.9333.h5\", compile=False)\n",
    "# model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v3_hovering_sinr/model.001-0.9316.h5\", compile=False)\n",
    "model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v2_hovering_novideo_sinr_ul/nn_v2_hovering_novideo_sinr_ul/model.006-1.5564.h5\", compile=False)\n",
    "# model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v2_hovering_novideo_sinr_dl/model.001-10.1139.h5\", compile=False)\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='binary_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'reliability': 'binary_crossentropy',\n",
    "                    'incorrectly_received': 'categorical_crossentropy',\n",
    "                    'delay_exceeded': 'binary_crossentropy',\n",
    "                    'queue_overflow': 'binary_crossentropy'},\n",
    "              metrics={'reliability': 'accuracy',\n",
    "                    'incorrectly_received': 'accuracy',\n",
    "                    'delay_exceeded': 'accuracy',\n",
    "                    'queue_overflow': 'accuracy'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uplink\n",
    "model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v4_multimodulation_video_nosinr_ul/model.005-0.1376.h5\", compile=False)\n",
    "# Downlink\n",
    "# model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v4_multimodulation_novideo_nosinr_dl/model.005-0.2016.h5\", compile=False)\n",
    "# Video\n",
    "# model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v4_multimodulation_video_nosinr_vid/model.005-0.2745.h5\", compile=False)\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'packet_state': 'categorical_crossentropy'},\n",
    "              metrics={'packet_state': 'accuracy'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using Taguchi Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "# test_data_df = pd.read_hdf(\"/media/research-student/One Touch/FANET Datasets/Stationary_Test_Dataset_NP10000_BPSK_6-5Mbps_downlink.h5\", \"Downlink\")\n",
    "test_data_df = pd.read_csv(\"/media/research-student/One Touch/FANET Datasets/Test_Dataset_NP10000_64QAM_65Mbps_Hovering_NoVideo_uplink.csv\")\n",
    "\n",
    "# Calculate mean and std dev of SINR\n",
    "test_data_df[['Mean_SINR',\"Std_Dev_SINR\"]]= test_data_df.apply(lambda row: sinr_lognormal_approx(row['Horizontal_Distance'],row['Height']),axis=1,result_type='expand')\n",
    "\n",
    "# Normalize inputs\n",
    "max_h_dist = 510\n",
    "max_height = 300\n",
    "max_num_members = 39\n",
    "max_bytes = 1500 # Should be 1144, but put 1145 just in case\n",
    "max_sending_int = 1000 # In ms\n",
    "max_mean_sinr = 521 # The max mean SINR calculated at (50,60) is 520.2907250903191\n",
    "max_std_dev_sinr = 252 # The max std dev SINR calculated at (50,60) is 251.44889082897834\n",
    "\n",
    "h_dists = test_data_df[\"Horizontal_Distance\"].values\n",
    "heights = test_data_df[\"Height\"].values\n",
    "num_members = test_data_df[\"Num_Members\"].values\n",
    "# pkt_sizes = test_data_df[\"Packet_Size\"].values\n",
    "sending_ints = test_data_df[\"Sending_Interval\"].values\n",
    "mean_sinr = test_data_df[\"Mean_SINR\"].values\n",
    "std_dev_sinr = test_data_df[\"Std_Dev_SINR\"].values\n",
    "\n",
    "norm_h_dist = [h_dist / max_h_dist for h_dist in h_dists]\n",
    "norm_height = [height / max_height for height in heights]\n",
    "norm_num_members = [num_member / max_num_members for num_member in num_members]\n",
    "# norm_pkt_size = [pkt_size / max_bytes for pkt_size in pkt_sizes]\n",
    "norm_sending_int = [sending_int / max_sending_int for sending_int in sending_ints]\n",
    "norm_mean_sinr = [m / max_mean_sinr for m in mean_sinr]\n",
    "norm_std_dev_sinr = [s / max_std_dev_sinr for s in std_dev_sinr]\n",
    "\n",
    "# For storing prediction results\n",
    "predicted_reliability = []\n",
    "predicted_incr_rcvd = [[],[],[],[],[],[],[],[]] # List of lists to store the prob of each incr rcvd probabilities\n",
    "predicted_delay_excd = []\n",
    "predicted_queue_overflow = []\n",
    "\n",
    "# Run inference\n",
    "# model_inputs = list(zip(norm_h_dist, norm_height, norm_num_members, norm_pkt_size, norm_sending_int))\n",
    "model_inputs = list(zip(norm_mean_sinr, norm_std_dev_sinr, norm_num_members, norm_sending_int))\n",
    "prediction = model.predict(model_inputs)\n",
    "# print(prediction)\n",
    "\n",
    "# Save the results to CSV\n",
    "test_data_df['Predicted_Reliability'] = [prob[1] for prob in prediction[0]]\n",
    "test_data_df['Predicted_Delay_Excd_Prob'] = [prob[1] for prob in prediction[2]]\n",
    "test_data_df['Predicted_Queue_Overflow_Prob'] = [prob[1] for prob in prediction[3]]\n",
    "test_data_df['Predicted_0_Incr_Rcvd'] = [prob[0] for prob in prediction[1]]\n",
    "test_data_df['Predicted_1_Incr_Rcvd'] = [prob[1] for prob in prediction[1]]\n",
    "test_data_df['Predicted_2_Incr_Rcvd'] = [prob[2] for prob in prediction[1]]\n",
    "test_data_df['Predicted_3_Incr_Rcvd'] = [prob[3] for prob in prediction[1]]\n",
    "test_data_df['Predicted_4_Incr_Rcvd'] = [prob[4] for prob in prediction[1]]\n",
    "test_data_df['Predicted_5_Incr_Rcvd'] = [prob[5] for prob in prediction[1]]\n",
    "test_data_df['Predicted_6_Incr_Rcvd'] = [prob[6] for prob in prediction[1]]\n",
    "test_data_df['Predicted_7_Incr_Rcvd'] = [prob[7] for prob in prediction[1]]\n",
    "\n",
    "test_data_df.to_csv(\"/media/research-student/One Touch/FANET Datasets/Test_Dataset_NP10000_64QAM_65Mbps_Hovering_NoVideo_uplink_RESULTS_nn.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using Taguchi Test Dataset (Single Multiclass Reliability and Failure Modes Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 3ms/step\n",
      "Reliability - Accuracy: 0.9875, MAE: 0.004869919483919781, MaxAE: 0.5801099083900452\n",
      "Failure Mode - Accuracy: 0.9895833333333334\n",
      "Queue Overflow - MeanAE: 0.00566187559881395, MaxAE: 0.21216222410202024\n",
      "Incorrectly Received - MeanAE: 0.005497301386443336, MaxAE: 0.060152588224411\n",
      "Delay Exceeded - MeanAE: 0.003621054483198103, MaxAE: 0.5017467755556106\n"
     ]
    }
   ],
   "source": [
    "# Set minimum probability for failure mode to be considered\n",
    "MIN_FAILURE_PROB = 0.01\n",
    "\n",
    "# Load test dataset\n",
    "test_data_df = pd.read_csv(\"/media/research-student/One Touch/FANET Datasets/Dataset_NP10000_MultiModulation_Hovering_NoVideo/Multi_Modulation_Test_Cases_2_Uplink.csv\")\n",
    "\n",
    "# Calculate mean and std dev of SINR\n",
    "test_data_df[['Mean_SINR',\"Std_Dev_SINR\"]] = test_data_df.apply(lambda row: sinr_lognormal_approx(row['Horizontal_Distance'],row['Height']),axis=1,result_type='expand')\n",
    "test_data_df[\"Mean_SINR\"] = test_data_df[\"Mean_SINR\"].apply(lambda x: 10*math.log10(x))\n",
    "test_data_df[\"Std_Dev_SINR\"] = test_data_df[\"Std_Dev_SINR\"].apply(lambda x: 10*math.log10(x))\n",
    "\n",
    "mean_sinr = test_data_df[\"Mean_SINR\"].values\n",
    "std_dev_sinr = test_data_df[\"Std_Dev_SINR\"].values\n",
    "\n",
    "# Normalize inputs\n",
    "max_mean_sinr = 10*math.log10(1123) # The max mean SINR calculated at (0,60) is 1122.743643457063 (linear)\n",
    "max_std_dev_sinr = 10*math.log10(466) # The max std dev SINR calculated at (0,60) is 465.2159856885714 (linear)\n",
    "min_mean_sinr = 10*math.log10(0.2) # The min mean SINR calculated at (1200,60) is 0.2251212887895188 (linear)\n",
    "min_std_dev_sinr = 10*math.log10(0.7) # The min std dev SINR calculated at (1200,300) is 0.7160093126585219 (linear)\n",
    "\n",
    "norm_mean_sinr = [2*(m-min_mean_sinr) / (max_mean_sinr-min_mean_sinr) - 1 for m in mean_sinr]\n",
    "norm_std_dev_sinr = [2*(s-min_std_dev_sinr) / (max_std_dev_sinr-min_std_dev_sinr) - 1 for s in std_dev_sinr]\n",
    "norm_uav_send_int = test_data_df[\"UAV_Sending_Interval\"].replace({10:-1, 20:-0.5, 40:0, 100:0.5, 1000:1}).values\n",
    "norm_modulation = test_data_df[\"Modulation\"].replace({\"BPSK\":1, \"QPSK\":0.3333, \"QAM16\":-0.3333, \"QAM64\":-1}).values\n",
    "\n",
    "# For storing prediction results\n",
    "predicted_reliability = []\n",
    "predicted_incr_rcvd = [] \n",
    "predicted_delay_excd = []\n",
    "predicted_queue_overflow = []\n",
    "\n",
    "# Run inference\n",
    "model_inputs = list(zip(norm_mean_sinr, norm_std_dev_sinr, norm_uav_send_int, norm_modulation))\n",
    "prediction = model.predict(model_inputs)\n",
    "# print(prediction)\n",
    "\n",
    "# Save the results to CSV\n",
    "test_data_df['Predicted_Reliability'] = [prob[0] for prob in prediction]\n",
    "test_data_df['Predicted_Queue_Overflow_Prob'] = [prob[1] for prob in prediction]\n",
    "test_data_df['Predicted_Incr_Rcvd_Prob'] = [prob[2] for prob in prediction]\n",
    "test_data_df['Predicted_Delay_Excd_Prob'] = [prob[3] for prob in prediction]\n",
    "\n",
    "test_data_df[\"Reliability_Class\"] = pd.cut(test_data_df[\"Reliability\"], bins=[-0.1,0.2,0.5,0.8,1], labels=[\"Low\", \"ModeratelyLow\", \"ModeratelyHigh\", \"High\"])\n",
    "test_data_df[\"Predicted_Reliability_Class\"] = pd.cut(test_data_df[\"Predicted_Reliability\"], bins=[-0.1,0.2,0.5,0.8,1], labels=[\"Low\", \"ModeratelyLow\", \"ModeratelyHigh\", \"High\"])\n",
    "test_data_df[\"Failure_Mode\"] = test_data_df[[\"Queue_Overflow_Prob\", \"Incorrectly_Rcvd_Prob\", \"Delay_Excd_Prob\"]].idxmax(axis=1)\n",
    "test_data_df[\"Predicted_Failure_Mode\"] = test_data_df[[\"Predicted_Queue_Overflow_Prob\", \"Predicted_Incr_Rcvd_Prob\", \"Predicted_Delay_Excd_Prob\"]].idxmax(axis=1)\n",
    "# Replace label for Failure Mode with \"None\" if none of the failure modes have a probability > 5%\n",
    "test_data_df.loc[(test_data_df[\"Queue_Overflow_Prob\"] < MIN_FAILURE_PROB) & (test_data_df[\"Incorrectly_Rcvd_Prob\"] < MIN_FAILURE_PROB) & (test_data_df[\"Delay_Excd_Prob\"] < MIN_FAILURE_PROB),[\"Failure_Mode\"]] = \"None\"\n",
    "test_data_df.loc[(test_data_df[\"Predicted_Queue_Overflow_Prob\"] < MIN_FAILURE_PROB) & (test_data_df[\"Predicted_Incr_Rcvd_Prob\"] < MIN_FAILURE_PROB) & (test_data_df[\"Predicted_Delay_Excd_Prob\"] < MIN_FAILURE_PROB),[\"Predicted_Failure_Mode\"]] = \"None\"\n",
    "\n",
    "# Compute the model accuracy and mean abs err\n",
    "failure_mode = test_data_df[\"Failure_Mode\"].replace({\"Queue_Overflow_Prob\":1, \"Incorrectly_Rcvd_Prob\":2, \"Delay_Excd_Prob\":3, \"None\":4})\n",
    "failure_mode_predicted = test_data_df[\"Predicted_Failure_Mode\"].replace({\"Predicted_Queue_Overflow_Prob\":1, \"Predicted_Incr_Rcvd_Prob\":2, \"Predicted_Delay_Excd_Prob\":3, \"None\":4})\n",
    "reliability_accuracy = accuracy_score(test_data_df[\"Reliability_Class\"], test_data_df[\"Predicted_Reliability_Class\"])\n",
    "failure_mode_accuracy = accuracy_score(failure_mode, failure_mode_predicted)\n",
    "reliability_mae = np.mean(abs(test_data_df['Reliability'].values - test_data_df['Predicted_Reliability'].values))\n",
    "queue_overflow_mae = np.mean(abs(test_data_df['Queue_Overflow_Prob'].values - test_data_df['Predicted_Queue_Overflow_Prob'].values))\n",
    "incr_rcvd_mae = np.mean(abs(test_data_df['Incorrectly_Rcvd_Prob'].values - test_data_df['Predicted_Incr_Rcvd_Prob'].values))\n",
    "delay_excd_mae = np.mean(abs(test_data_df['Delay_Excd_Prob'].values - test_data_df['Predicted_Delay_Excd_Prob'].values))\n",
    "reliability_maxae = np.max(abs(test_data_df['Reliability'].values - test_data_df['Predicted_Reliability'].values))\n",
    "queue_overflow_maxae = np.max(abs(test_data_df['Queue_Overflow_Prob'].values - test_data_df['Predicted_Queue_Overflow_Prob'].values))\n",
    "incr_rcvd_maxae = np.max(abs(test_data_df['Incorrectly_Rcvd_Prob'].values - test_data_df['Predicted_Incr_Rcvd_Prob'].values))\n",
    "delay_excd_maxae = np.max(abs(test_data_df['Delay_Excd_Prob'].values - test_data_df['Predicted_Delay_Excd_Prob'].values))\n",
    "\n",
    "# Print results\n",
    "print(\"Reliability - Accuracy: {}, MAE: {}, MaxAE: {}\".format(reliability_accuracy, reliability_mae, reliability_maxae))\n",
    "print(\"Failure Mode - Accuracy: {}\".format(failure_mode_accuracy))\n",
    "print(\"Queue Overflow - MeanAE: {}, MaxAE: {}\".format(queue_overflow_mae, queue_overflow_maxae))\n",
    "print(\"Incorrectly Received - MeanAE: {}, MaxAE: {}\".format(incr_rcvd_mae, incr_rcvd_maxae))\n",
    "print(\"Delay Exceeded - MeanAE: {}, MaxAE: {}\".format(delay_excd_mae, delay_excd_maxae))\n",
    "\n",
    "# Save results to file\n",
    "# test_data_df.to_csv(\"/media/research-student/One Touch/FANET Datasets/Dataset_NP10000_MultiModulation_Hovering_NoVideo/Multi_Modulation_Test_Cases_2_Downlink_RESULTS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004926743822818463"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.00566187559881395, 0.005497301386443336, 0.003621054483198103])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using Taguchi Test Dataset (No SINR) (Single Multiclass Reliability and Failure Modes Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 2ms/step\n",
      "Reliability - Accuracy: 0.9927083333333333, MAE: 0.002744380153625911, MaxAE: 0.49762199378013616\n",
      "Failure Mode - Accuracy: 0.9916666666666667\n",
      "Queue Overflow - MeanAE: 0.004665819713344343, MaxAE: 0.16435093121528627\n",
      "Incorrectly Received - MeanAE: 0.0007674163412448138, MaxAE: 0.003962640689872205\n",
      "Delay Exceeded - MeanAE: 0.006224850996109199, MaxAE: 0.4828524098396301\n"
     ]
    }
   ],
   "source": [
    "# Set minimum probability for failure mode to be considered\n",
    "MIN_FAILURE_PROB = 0.01\n",
    "\n",
    "# Load test dataset\n",
    "test_data_df = pd.read_csv(\"/media/research-student/One Touch/FANET Datasets/Dataset_NP10000_MultiModulation_Hovering_Video/Test/Multi_Modulation_Test_Cases_1_Uplink.csv\")\n",
    "\n",
    "# Get height and h_dist inputs\n",
    "h_dist = test_data_df[\"Horizontal_Distance\"].values\n",
    "height = test_data_df[\"Height\"].values\n",
    "\n",
    "# Normalize inputs\n",
    "max_height = 300\n",
    "min_height = 60\n",
    "max_h_dist = 1200\n",
    "min_h_dist = 0\n",
    "\n",
    "norm_h_dist = [2*(h_d-min_h_dist) / (max_h_dist-min_h_dist) - 1 for h_d in h_dist]\n",
    "norm_height = [2*(h-min_height) / (max_height-min_height) - 1 for h in height]\n",
    "norm_uav_send_int = test_data_df[\"UAV_Sending_Interval\"].replace({10:-1, 20:-0.5, 40:0, 100:0.5, 1000:1}).values\n",
    "norm_modulation = test_data_df[\"Modulation\"].replace({\"BPSK\":1, \"QPSK\":0.3333, \"QAM16\":-0.3333, \"QAM64\":-1}).values\n",
    "\n",
    "# For storing prediction results\n",
    "predicted_reliability = []\n",
    "predicted_incr_rcvd = [] \n",
    "predicted_delay_excd = []\n",
    "predicted_queue_overflow = []\n",
    "\n",
    "# Run inference\n",
    "model_inputs = list(zip(norm_h_dist, norm_height, norm_uav_send_int, norm_modulation))\n",
    "prediction = model.predict(model_inputs)\n",
    "# print(prediction)\n",
    "\n",
    "# Save the results to CSV\n",
    "test_data_df['Predicted_Reliability'] = [prob[0] for prob in prediction]\n",
    "test_data_df['Predicted_Queue_Overflow_Prob'] = [prob[1] for prob in prediction]\n",
    "test_data_df['Predicted_Incr_Rcvd_Prob'] = [prob[2] for prob in prediction]\n",
    "test_data_df['Predicted_Delay_Excd_Prob'] = [prob[3] for prob in prediction]\n",
    "\n",
    "test_data_df[\"Reliability_Class\"] = pd.cut(test_data_df[\"Reliability\"], bins=[-0.1,0.2,0.5,0.8,1], labels=[\"Low\", \"ModeratelyLow\", \"ModeratelyHigh\", \"High\"])\n",
    "test_data_df[\"Predicted_Reliability_Class\"] = pd.cut(test_data_df[\"Predicted_Reliability\"], bins=[-0.1,0.2,0.5,0.8,1], labels=[\"Low\", \"ModeratelyLow\", \"ModeratelyHigh\", \"High\"])\n",
    "test_data_df[\"Failure_Mode\"] = test_data_df[[\"Queue_Overflow_Prob\", \"Incorrectly_Rcvd_Prob\", \"Delay_Excd_Prob\"]].idxmax(axis=1)\n",
    "test_data_df[\"Predicted_Failure_Mode\"] = test_data_df[[\"Predicted_Queue_Overflow_Prob\", \"Predicted_Incr_Rcvd_Prob\", \"Predicted_Delay_Excd_Prob\"]].idxmax(axis=1)\n",
    "# Replace label for Failure Mode with \"None\" if none of the failure modes have a probability > 5%\n",
    "test_data_df.loc[(test_data_df[\"Queue_Overflow_Prob\"] < MIN_FAILURE_PROB) & (test_data_df[\"Incorrectly_Rcvd_Prob\"] < MIN_FAILURE_PROB) & (test_data_df[\"Delay_Excd_Prob\"] < MIN_FAILURE_PROB),[\"Failure_Mode\"]] = \"None\"\n",
    "test_data_df.loc[(test_data_df[\"Predicted_Queue_Overflow_Prob\"] < MIN_FAILURE_PROB) & (test_data_df[\"Predicted_Incr_Rcvd_Prob\"] < MIN_FAILURE_PROB) & (test_data_df[\"Predicted_Delay_Excd_Prob\"] < MIN_FAILURE_PROB),[\"Predicted_Failure_Mode\"]] = \"None\"\n",
    "\n",
    "# Compute the model accuracy and mean abs err\n",
    "failure_mode = test_data_df[\"Failure_Mode\"].replace({\"Queue_Overflow_Prob\":1, \"Incorrectly_Rcvd_Prob\":2, \"Delay_Excd_Prob\":3, \"None\":4})\n",
    "failure_mode_predicted = test_data_df[\"Predicted_Failure_Mode\"].replace({\"Predicted_Queue_Overflow_Prob\":1, \"Predicted_Incr_Rcvd_Prob\":2, \"Predicted_Delay_Excd_Prob\":3, \"None\":4})\n",
    "reliability_accuracy = accuracy_score(test_data_df[\"Reliability_Class\"], test_data_df[\"Predicted_Reliability_Class\"])\n",
    "failure_mode_accuracy = accuracy_score(failure_mode, failure_mode_predicted)\n",
    "reliability_mae = np.mean(abs(test_data_df['Reliability'].values - test_data_df['Predicted_Reliability'].values))\n",
    "queue_overflow_mae = np.mean(abs(test_data_df['Queue_Overflow_Prob'].values - test_data_df['Predicted_Queue_Overflow_Prob'].values))\n",
    "incr_rcvd_mae = np.mean(abs(test_data_df['Incorrectly_Rcvd_Prob'].values - test_data_df['Predicted_Incr_Rcvd_Prob'].values))\n",
    "delay_excd_mae = np.mean(abs(test_data_df['Delay_Excd_Prob'].values - test_data_df['Predicted_Delay_Excd_Prob'].values))\n",
    "reliability_maxae = np.max(abs(test_data_df['Reliability'].values - test_data_df['Predicted_Reliability'].values))\n",
    "queue_overflow_maxae = np.max(abs(test_data_df['Queue_Overflow_Prob'].values - test_data_df['Predicted_Queue_Overflow_Prob'].values))\n",
    "incr_rcvd_maxae = np.max(abs(test_data_df['Incorrectly_Rcvd_Prob'].values - test_data_df['Predicted_Incr_Rcvd_Prob'].values))\n",
    "delay_excd_maxae = np.max(abs(test_data_df['Delay_Excd_Prob'].values - test_data_df['Predicted_Delay_Excd_Prob'].values))\n",
    "\n",
    "# Print results\n",
    "print(\"Reliability - Accuracy: {}, MAE: {}, MaxAE: {}\".format(reliability_accuracy, reliability_mae, reliability_maxae))\n",
    "print(\"Failure Mode - Accuracy: {}\".format(failure_mode_accuracy))\n",
    "print(\"Queue Overflow - MeanAE: {}, MaxAE: {}\".format(queue_overflow_mae, queue_overflow_maxae))\n",
    "print(\"Incorrectly Received - MeanAE: {}, MaxAE: {}\".format(incr_rcvd_mae, incr_rcvd_maxae))\n",
    "print(\"Delay Exceeded - MeanAE: {}, MaxAE: {}\".format(delay_excd_mae, delay_excd_maxae))\n",
    "\n",
    "# Save results to file\n",
    "test_data_df.to_csv(\"/media/research-student/One Touch/FANET Datasets/Dataset_NP10000_MultiModulation_Hovering_Video/Test/Multi_Modulation_Test_Cases_1_Uplink_NN_NoSINR_RESULTS.csv\")\n",
    "\n",
    "# Get the results at the \"drop\" only\n",
    "test_data_drop_df = test_data_df.loc[(test_data_df[\"Reliability\"] < 0.99) & (test_data_df[\"Reliability\"] > 0.01)]\n",
    "    # Compute the model accuracy and mean abs err\n",
    "failure_mode = test_data_drop_df[\"Failure_Mode\"].replace({\"Queue_Overflow_Prob\":1, \"Incorrectly_Rcvd_Prob\":2, \"Delay_Excd_Prob\":3, \"None\":4})\n",
    "failure_mode_predicted = test_data_drop_df[\"Predicted_Failure_Mode\"].replace({\"Predicted_Queue_Overflow_Prob\":1, \"Predicted_Incr_Rcvd_Prob\":2, \"Predicted_Delay_Excd_Prob\":3, \"None\":4})\n",
    "reliability_accuracy = accuracy_score(test_data_drop_df[\"Reliability_Class\"], test_data_drop_df[\"Predicted_Reliability_Class\"])\n",
    "failure_mode_accuracy = accuracy_score(failure_mode, failure_mode_predicted)\n",
    "reliability_mae = np.mean(abs(test_data_drop_df['Reliability'].values - test_data_drop_df['Predicted_Reliability'].values))\n",
    "queue_overflow_mae = np.mean(abs(test_data_drop_df['Queue_Overflow_Prob'].values - test_data_drop_df['Predicted_Queue_Overflow_Prob'].values))\n",
    "incr_rcvd_mae = np.mean(abs(test_data_drop_df['Incorrectly_Rcvd_Prob'].values - test_data_drop_df['Predicted_Incr_Rcvd_Prob'].values))\n",
    "delay_excd_mae = np.mean(abs(test_data_drop_df['Delay_Excd_Prob'].values - test_data_drop_df['Predicted_Delay_Excd_Prob'].values))\n",
    "reliability_maxae = np.max(abs(test_data_drop_df['Reliability'].values - test_data_drop_df['Predicted_Reliability'].values))\n",
    "queue_overflow_maxae = np.max(abs(test_data_drop_df['Queue_Overflow_Prob'].values - test_data_drop_df['Predicted_Queue_Overflow_Prob'].values))\n",
    "incr_rcvd_maxae = np.max(abs(test_data_drop_df['Incorrectly_Rcvd_Prob'].values - test_data_drop_df['Predicted_Incr_Rcvd_Prob'].values))\n",
    "delay_excd_maxae = np.max(abs(test_data_drop_df['Delay_Excd_Prob'].values - test_data_drop_df['Predicted_Delay_Excd_Prob'].values))\n",
    "print(\"RESULTS AT DROP REGION =====================================\")\n",
    "print(\"Reliability - Accuracy: {}, MAE: {}, MaxAE: {}\".format(reliability_accuracy, reliability_mae, reliability_maxae))\n",
    "print(\"Failure Mode - Accuracy: {}\".format(failure_mode_accuracy))\n",
    "print(\"Queue Overflow - MeanAE: {}, MaxAE: {}\".format(queue_overflow_mae, queue_overflow_maxae))\n",
    "print(\"Incorrectly Received - MeanAE: {}, MaxAE: {}\".format(incr_rcvd_mae, incr_rcvd_maxae))\n",
    "print(\"Delay Exceeded - MeanAE: {}, MaxAE: {}\".format(delay_excd_mae, delay_excd_maxae))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for specific experiment no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n",
      "[0.99999994, 0.99999994, 0.99999994, 0.99999994, 0.99999994, 0.9999953, 0.9997854, 0.99973774, 0.9927658, 0.35241845, 0.10420601, 0.0017122723, 2.3735553e-05, 1.0709967e-06, 7.4764095e-08, 3.426935e-08, 1.8313905e-08, 1.5328803e-08, 3.7511794e-09, 1.007431e-09, 5.522583e-10, 3.1600253e-10, 1.91892e-10, 1.2267251e-10, 8.191211e-11, 5.1374297e-11, 3.3649573e-11, 2.2897871e-11, 1.6116471e-11, 1.1688064e-11, 3.3484007e-11, 1.2179403e-10, 3.05861e-10, 6.8554873e-10, 1.392557e-09, 2.5971767e-09, 4.49703e-09, 7.082125e-09, 1.0354178e-08, 1.4484203e-08, 1.9491704e-08, 2.0031996e-08, 2.0026189e-08, 2.0023096e-08, 2.0022332e-08, 2.0023592e-08, 2.002661e-08, 2.0031042e-08, 1.9356996e-08]\n"
     ]
    }
   ],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "# h_dists = [50, 100, 125, 150, 175, 200, 225, 250, 300, 350, 400, 450, 500] # In m\n",
    "# h_dists = [150, 175, 200, 225, 250, 300, 350, 400, 450, 500] # In m\n",
    "# h_dists = [50, 75, 100, 112.5, 125, 137.5, 150, 162.5, 175, 187.5, 200, 212.5, 225, 237.5, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475, 500] # In m\n",
    "# h_dists = [75, 112.5, 137.5, 162.5, 187.5, 212.5, 237.5, 275, 325, 375, 425, 475] # In m\n",
    "# h_dists = [50, 100, 112.5, 125, 137.5, 150, 162.5, 175, 187.5, 200, 212.5, 225, 237.5, 250, 300, 350, 400, 450, 500] # In m\n",
    "h_dists = np.linspace(20,500,num=49,endpoint=True)\n",
    "# h_dist = 30\n",
    "height = 70 # In m\n",
    "# heights = np.linspace(60,300,num=25,endpoint=True)\n",
    "# u2g_dist = [math.sqrt(h_dist**2 + height**2) for h_dist in h_dists]\n",
    "num_members = 7\n",
    "pkt_size = 50 # In bytes\n",
    "sending_int = 10 # In ms\n",
    "snr_moments = [sinr_lognormal_approx(h_dist, height) for h_dist in h_dists]\n",
    "# snr_moments = [sinr_lognormal_approx(h_dist, height) for height in heights]\n",
    "mean_snr = [x[0] for x in snr_moments]\n",
    "std_dev_snr = [x[1] for x in snr_moments]\n",
    "\n",
    "# max_h_dist = 550\n",
    "max_h_dist = 500\n",
    "max_height = 300\n",
    "max_num_members = 39\n",
    "# max_num_members = 7\n",
    "max_bytes = 1500 # Should be 1144, but put 1145 just in case\n",
    "max_sending_int = 1000 # In ms\n",
    "# max_mean_sinr = 6193 # The max mean SINR calculated at (0,24) is 668.1670595316114\n",
    "# max_std_dev_sinr = 2780 # The max std dev SINR calculated at (0,24) is 578.6543714478831\n",
    "max_mean_sinr = 521 # The max mean SINR calculated at (0,24) is 520.2907250903191\n",
    "max_std_dev_sinr = 252 # The max std dev SINR calculated at (0,24) is 251.44889082897834\n",
    "\n",
    "# norm_h_dist = [h_dist / max_h_dist for h_dist in h_dists]\n",
    "# norm_height = height / max_height\n",
    "norm_num_members = num_members / max_num_members\n",
    "norm_pkt_size = pkt_size / max_bytes\n",
    "norm_sending_int = sending_int / max_sending_int\n",
    "norm_mean_snr = [m / max_mean_sinr for m in mean_snr]\n",
    "norm_std_dev_snr = [s / max_std_dev_sinr for s in std_dev_snr]\n",
    "\n",
    "model_inputs = list(zip(norm_mean_snr, norm_std_dev_snr, repeat(norm_num_members), repeat(norm_sending_int)))\n",
    "# model_inputs = list(zip(norm_mean_snr, norm_std_dev_snr, repeat(norm_num_members), repeat(norm_pkt_size), repeat(norm_sending_int)))\n",
    "# model_inputs = list(zip(norm_h_dist, repeat(norm_height), repeat(norm_num_members), repeat(norm_pkt_size), repeat(norm_sending_int)))\n",
    "# model_input = [norm_h_dist,norm_height,norm_num_members,norm_pkt_size,norm_sending_int]\n",
    "\n",
    "prediction = model.predict(model_inputs)\n",
    "# print(prediction)\n",
    "print([p[1] for p in prediction[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for single case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "[array([[4.8760677e-04, 9.9951243e-01]], dtype=float32), array([[0.49350592, 0.2441833 , 0.13018824, 0.07528864, 0.03120731,\n",
      "        0.01573893, 0.0084472 , 0.0014404 ]], dtype=float32), array([[9.9970812e-01, 2.9182597e-04]], dtype=float32), array([[1.0000000e+00, 2.2164258e-20]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "h_dist = 149.966715057041 # In m\n",
    "height = 24 # In m\n",
    "num_members = 15\n",
    "pkt_size = 360 # In bytes\n",
    "sending_int = 520 # In ms\n",
    "\n",
    "max_h_dist = 550\n",
    "max_height = 120\n",
    "max_num_members = 39\n",
    "max_bytes = 1145 # Should be 1144, but put 1145 just in case\n",
    "max_sending_int = 1000 # In ms\n",
    "\n",
    "norm_h_dist = h_dist / max_h_dist\n",
    "norm_height = height / max_height\n",
    "norm_num_members = num_members / max_num_members\n",
    "norm_pkt_size = pkt_size / max_bytes\n",
    "norm_sending_int = sending_int / max_sending_int\n",
    "\n",
    "model_inputs = [norm_h_dist, norm_height, norm_num_members, norm_pkt_size, norm_sending_int]\n",
    "# model_input = [norm_h_dist,norm_height,norm_num_members,norm_pkt_size,norm_sending_int]\n",
    "\n",
    "prediction = model.predict([model_inputs])\n",
    "print(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "# kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (len(X_train_all))\n",
    "# bias_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (len(X_train_all))\n",
    "\n",
    "inputs = Input(shape=(5,))\n",
    "base = tfp.layers.DenseFlipout(50, activation='relu')(inputs)\n",
    "base = tfp.layers.DenseFlipout(25, activation='relu')(base)\n",
    "base = tfp.layers.DenseFlipout(10, activation='relu')(base)\n",
    "reliability_hl = tfp.layers.DenseFlipout(10, activation='relu')(base)\n",
    "incr_rcvd_out_hl = tfp.layers.DenseFlipout(10, activation='relu')(base)\n",
    "delay_excd_hl = tfp.layers.DenseFlipout(10, activation='relu')(base)\n",
    "queue_overflow_hl = tfp.layers.DenseFlipout(10, activation='relu')(base)\n",
    "reliability_out = tfp.layers.DenseFlipout(2, activation='softmax', name='reliability')(reliability_hl)\n",
    "incr_rcvd_out = tfp.layers.DenseFlipout(8, activation='softmax', name='incorrectly_received')(incr_rcvd_out_hl)\n",
    "delay_excd_out = tfp.layers.DenseFlipout(2, activation='softmax', name='delay_exceeded')(delay_excd_hl)\n",
    "queue_overflow_out = tfp.layers.DenseFlipout(2, activation='softmax', name='queue_overflow')(queue_overflow_hl)\n",
    "model = Model(inputs=inputs, outputs = [reliability_out, incr_rcvd_out, delay_excd_out, queue_overflow_out])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'reliability': 'binary_crossentropy',\n",
    "                    'incorrectly_received': 'categorical_crossentropy',\n",
    "                    'delay_exceeded': 'binary_crossentropy',\n",
    "                    'queue_overflow': 'binary_crossentropy'},\n",
    "              metrics={'reliability': 'accuracy',\n",
    "                    'incorrectly_received': 'accuracy',\n",
    "                    'delay_exceeded': 'accuracy',\n",
    "                    'queue_overflow': 'accuracy'},)\n",
    "\n",
    "model.load_weights(\"/home/research-student/omnet-fanet/nn_checkpoints/bnn_v2_hovering_novideo_sinr/model.030-0.5846.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for specific experiment no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 820ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[0.9999999403953552, 0.9999996423721313, 0.999481737613678, 0.015482890419661999, 6.9508882916125e-06, 1.7113026160586742e-06, 6.088038162488374e-07, 3.168134696807101e-07, 1.918535019740375e-07, 1.4957397809212125e-07, 8.649522698078727e-08, 6.263967122777103e-08, 4.356937211014156e-08, 2.0562852753869265e-08, 2.3365576140577105e-08, 3.338783116646482e-08, 3.976461826482591e-08, 4.147893051253959e-08, 2.061490889104789e-08]\n"
     ]
    }
   ],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "# h_dists = [50, 100, 125, 150, 175, 200, 225, 250, 300, 350, 400, 450, 500] # In m\n",
    "# h_dists = [150, 175, 200, 225, 250, 300, 350, 400, 450, 500] # In m\n",
    "# h_dists = [50, 75, 100, 112.5, 125, 137.5, 150, 162.5, 175, 187.5, 200, 212.5, 225, 237.5, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475, 500] # In m\n",
    "# h_dists = [75, 112.5, 137.5, 162.5, 187.5, 212.5, 237.5, 275, 325, 375, 425, 475] # In m\n",
    "# h_dists = [50, 100, 112.5, 125, 137.5, 150, 162.5, 175, 187.5, 200, 212.5, 225, 237.5, 250, 300, 350, 400, 450, 500] # In m\n",
    "h_dist = np.linspace(20,500,num=49,endpoint=True)\n",
    "height = 60 # In m\n",
    "u2g_dist = [math.sqrt(h_dist**2 + height**2) for h_dist in h_dists]\n",
    "num_members = 7\n",
    "pkt_size = 50 # In bytes\n",
    "sending_int = 10 # In ms\n",
    "snr_moments = [sinr_lognormal_approx(h_dist, height) for h_dist in h_dists]\n",
    "mean_snr = [x[0] for x in snr_moments]\n",
    "std_dev_snr = [x[1] for x in snr_moments]\n",
    "\n",
    "# max_h_dist = 550\n",
    "max_h_dist = 500\n",
    "max_height = 300\n",
    "max_num_members = 39\n",
    "# max_num_members = 7\n",
    "max_bytes = 1500 # Should be 1144, but put 1145 just in case\n",
    "max_sending_int = 1000 # In ms\n",
    "# max_mean_sinr = 6193 # The max mean SINR calculated at (0,24) is 668.1670595316114\n",
    "# max_std_dev_sinr = 2780 # The max std dev SINR calculated at (0,24) is 578.6543714478831\n",
    "max_mean_sinr = 521 # The max mean SINR calculated at (50,60) is 520.2907250903191\n",
    "max_std_dev_sinr = 252 # The max std dev SINR calculated at (50,60) is 251.44889082897834\n",
    "\n",
    "norm_h_dist = [h_dist / max_h_dist for h_dist in h_dists]\n",
    "norm_height = height / max_height\n",
    "norm_num_members = num_members / max_num_members\n",
    "norm_pkt_size = pkt_size / max_bytes\n",
    "norm_sending_int = sending_int / max_sending_int\n",
    "norm_mean_snr = [m / max_mean_sinr for m in mean_snr]\n",
    "norm_std_dev_snr = [s / max_std_dev_sinr for s in std_dev_snr]\n",
    "\n",
    "model_inputs = list(zip(norm_mean_snr, norm_std_dev_snr, repeat(norm_num_members), repeat(norm_pkt_size), repeat(norm_sending_int)))\n",
    "# model_inputs = list(zip(norm_h_dist, repeat(norm_height), repeat(norm_num_members), repeat(norm_pkt_size), repeat(norm_sending_int)))\n",
    "# model_input = [norm_h_dist,norm_height,norm_num_members,norm_pkt_size,norm_sending_int]\n",
    "\n",
    "\n",
    "prediction = []\n",
    "for i in range(100):\n",
    "    prediction.append([p[1] for p in model.predict(model_inputs)[0]])\n",
    "prediction = np.array(prediction).mean(axis=0)\n",
    "# print([p[1] for p in prediction[0]])\n",
    "print(prediction.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2 NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v2-20032023/final_model.h5\", compile=False)\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='binary_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'reliability': 'binary_crossentropy',\n",
    "                    'incorrectly_received': 'categorical_crossentropy',\n",
    "                    'delay_exceeded': 'binary_crossentropy',\n",
    "                    'queue_overflow': 'binary_crossentropy'},\n",
    "              metrics={'reliability': 'accuracy',\n",
    "                    'incorrectly_received': 'accuracy',\n",
    "                    'delay_exceeded': 'accuracy',\n",
    "                    'queue_overflow': 'accuracy'},)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using Taguchi Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "test_data_df = pd.read_hdf(\"/media/research-student/One Touch/FANET Datasets/Stationary_Test_Dataset_NP10000_BPSK_6-5Mbps_downlink.h5\", \"Downlink\")\n",
    "\n",
    "# Normalize inputs\n",
    "max_h_dist = 550\n",
    "max_height = 120\n",
    "max_num_members = 39\n",
    "max_bytes = 1145 # Should be 1144, but put 1145 just in case\n",
    "max_sending_int = 1000 # In ms\n",
    "\n",
    "h_dists = test_data_df[\"Horizontal_Distance\"].values\n",
    "heights = test_data_df[\"Height\"].values\n",
    "num_members = test_data_df[\"Num_Members\"].values\n",
    "pkt_sizes = test_data_df[\"Packet_Size\"].values\n",
    "sending_ints = test_data_df[\"Sending_Interval\"].values\n",
    "\n",
    "norm_h_dist = [h_dist / max_h_dist for h_dist in h_dists]\n",
    "norm_height = [height / max_height for height in heights]\n",
    "norm_num_members = [num_member / max_num_members for num_member in num_members]\n",
    "norm_pkt_size = [pkt_size / max_bytes for pkt_size in pkt_sizes]\n",
    "norm_sending_int = [sending_int / max_sending_int for sending_int in sending_ints]\n",
    "\n",
    "# For storing prediction results\n",
    "predicted_reliability = []\n",
    "predicted_incr_rcvd = [[],[],[],[],[],[],[],[]] # List of lists to store the prob of each incr rcvd probabilities\n",
    "predicted_delay_excd = []\n",
    "predicted_queue_overflow = []\n",
    "\n",
    "# Run inference\n",
    "model_inputs = list(zip(norm_h_dist, norm_height, norm_num_members, norm_pkt_size, norm_sending_int))\n",
    "prediction = model.predict(model_inputs)\n",
    "# print(prediction)\n",
    "\n",
    "# Save the results to CSV\n",
    "test_data_df['Predicted_Reliability'] = [prob[1] for prob in prediction[0]]\n",
    "test_data_df['Predicted_Delay_Excd_Prob'] = [prob[1] for prob in prediction[2]]\n",
    "test_data_df['Predicted_Queue_Overflow_Prob'] = [prob[1] for prob in prediction[3]]\n",
    "test_data_df['Predicted_0_Incr_Rcvd'] = [prob[0] for prob in prediction[1]]\n",
    "test_data_df['Predicted_1_Incr_Rcvd'] = [prob[1] for prob in prediction[1]]\n",
    "test_data_df['Predicted_2_Incr_Rcvd'] = [prob[2] for prob in prediction[1]]\n",
    "test_data_df['Predicted_3_Incr_Rcvd'] = [prob[3] for prob in prediction[1]]\n",
    "test_data_df['Predicted_4_Incr_Rcvd'] = [prob[4] for prob in prediction[1]]\n",
    "test_data_df['Predicted_5_Incr_Rcvd'] = [prob[5] for prob in prediction[1]]\n",
    "test_data_df['Predicted_6_Incr_Rcvd'] = [prob[6] for prob in prediction[1]]\n",
    "test_data_df['Predicted_7_Incr_Rcvd'] = [prob[7] for prob in prediction[1]]\n",
    "\n",
    "test_data_df.to_csv(\"Stationary_Test_Dataset_NP10000_BPSK_6-5Mbps_downlink_RESULTS_nnv2.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2-2 NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 14:51:19.033354: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 14:51:19.163811: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-21 14:51:19.168666: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:19.168682: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-21 14:51:19.191290: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-21 14:51:19.646947: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:19.647006: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:19.647012: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-21 14:51:20.390485: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:20.390528: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:20.390572: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:20.390615: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:20.390642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:20.390668: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:20.390694: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:20.390721: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-21 14:51:20.390726: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-21 14:51:20.390949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/nn_v2-2_21032023/model.010-2.0131.h5\", compile=False)\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='binary_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'reliability': 'binary_crossentropy',\n",
    "                    'incorrectly_received': 'categorical_crossentropy',\n",
    "                    'delay_exceeded': 'binary_crossentropy',\n",
    "                    'queue_overflow': 'binary_crossentropy'},\n",
    "              metrics={'reliability': 'accuracy',\n",
    "                    'incorrectly_received': 'accuracy',\n",
    "                    'delay_exceeded': 'accuracy',\n",
    "                    'queue_overflow': 'accuracy'},)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using Taguchi Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "test_data_df = pd.read_hdf(\"/media/research-student/One Touch/FANET Datasets/Stationary_Test_Dataset_NP10000_BPSK_6-5Mbps_downlink.h5\", \"Downlink\")\n",
    "\n",
    "# Normalize inputs\n",
    "max_h_dist = 550\n",
    "max_height = 120\n",
    "max_num_members = 39\n",
    "max_bytes = 1145 # Should be 1144, but put 1145 just in case\n",
    "max_sending_int = 1000 # In ms\n",
    "\n",
    "h_dists = test_data_df[\"Horizontal_Distance\"].values\n",
    "heights = test_data_df[\"Height\"].values\n",
    "num_members = test_data_df[\"Num_Members\"].values\n",
    "pkt_sizes = test_data_df[\"Packet_Size\"].values\n",
    "sending_ints = test_data_df[\"Sending_Interval\"].values\n",
    "\n",
    "norm_h_dist = [h_dist / max_h_dist for h_dist in h_dists]\n",
    "norm_height = [height / max_height for height in heights]\n",
    "norm_num_members = [num_member / max_num_members for num_member in num_members]\n",
    "norm_pkt_size = [pkt_size / max_bytes for pkt_size in pkt_sizes]\n",
    "norm_sending_int = [sending_int / max_sending_int for sending_int in sending_ints]\n",
    "\n",
    "# For storing prediction results\n",
    "predicted_reliability = []\n",
    "predicted_incr_rcvd = [[],[],[],[],[],[],[],[]] # List of lists to store the prob of each incr rcvd probabilities\n",
    "predicted_delay_excd = []\n",
    "predicted_queue_overflow = []\n",
    "\n",
    "# Run inference\n",
    "model_inputs = list(zip(norm_h_dist, norm_height, norm_num_members, norm_pkt_size, norm_sending_int))\n",
    "prediction = model.predict(model_inputs)\n",
    "# print(prediction)\n",
    "\n",
    "# Save the results to CSV\n",
    "test_data_df['Predicted_Reliability'] = [prob[1] for prob in prediction[0]]\n",
    "test_data_df['Predicted_Delay_Excd_Prob'] = [prob[1] for prob in prediction[2]]\n",
    "test_data_df['Predicted_Queue_Overflow_Prob'] = [prob[1] for prob in prediction[3]]\n",
    "test_data_df['Predicted_0_Incr_Rcvd'] = [prob[0] for prob in prediction[1]]\n",
    "test_data_df['Predicted_1_Incr_Rcvd'] = [prob[1] for prob in prediction[1]]\n",
    "test_data_df['Predicted_2_Incr_Rcvd'] = [prob[2] for prob in prediction[1]]\n",
    "test_data_df['Predicted_3_Incr_Rcvd'] = [prob[3] for prob in prediction[1]]\n",
    "test_data_df['Predicted_4_Incr_Rcvd'] = [prob[4] for prob in prediction[1]]\n",
    "test_data_df['Predicted_5_Incr_Rcvd'] = [prob[5] for prob in prediction[1]]\n",
    "test_data_df['Predicted_6_Incr_Rcvd'] = [prob[6] for prob in prediction[1]]\n",
    "test_data_df['Predicted_7_Incr_Rcvd'] = [prob[7] for prob in prediction[1]]\n",
    "\n",
    "test_data_df.to_csv(\"Stationary_Test_Dataset_NP10000_BPSK_6-5Mbps_downlink_RESULTS_nn_V2-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"/media/research-student/One Touch/FANET Datasets/Dataset_NP10000_64QAM_65Mbps_Hovering/Dataset_NP10000_64QAM_65Mbps_Hovering_8UAVs_processed_downlink.h5\", key=\"8_UAVs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "autoencoder = keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/ae_multimodulation_novideo_sinr_ul/final_model.h5\", compile=False)\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics='mse')\n",
    "# Get the encoder part of the autoencoder\n",
    "encoder_layer = autoencoder.get_layer('latent')\n",
    "encoder = Model(inputs=autoencoder.input, outputs=encoder_layer.output)\n",
    "\n",
    "out = encoder.predict([[1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8]])\n",
    "out = np.concatenate((out, encoder.predict([[1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8]])), axis=0)\n",
    "\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Concatenate\n",
    "import numpy as np\n",
    "\n",
    "autoencoder = keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/ae_multimodulation_novideo_sinr_ul/final_model.h5\", compile=False)\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics='mse')\n",
    "# Get the encoder part of the autoencoder\n",
    "encoder_input = autoencoder.get_layer('encoder')\n",
    "encoder_layer = autoencoder.get_layer('latent')\n",
    "# Freeze the encoder part\n",
    "encoder_input.trainable = False\n",
    "encoder_layer.trainable = False\n",
    "# Create hierarchical model\n",
    "haenn = keras.Sequential(\n",
    "    [\n",
    "        Input(shape=(4,)),\n",
    "        encoder_input,\n",
    "        encoder_layer,\n",
    "        Dense(25, activation='relu', name='classifier_1'),\n",
    "        BatchNormalization(name='batch_norm_1'), \n",
    "        Dense(10, activation='relu', name='classifier_2'),\n",
    "        BatchNormalization(name='batch_norm_2'), \n",
    "        Dense(1, activation='linear', name='indicator')\n",
    "    ]\n",
    ")\n",
    "\n",
    "haenn.compile(optimizer='adam', \n",
    "              loss={'indicator': 'mae'},\n",
    "              metrics={'indicator': 'accuracy'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Throughput NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/30 [>.............................] - ETA: 3s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 2ms/step\n",
      "Throughput - MAE: 231.81302361454303, MaxAE: 5248.842034022119\n"
     ]
    }
   ],
   "source": [
    "# Load throughput prediction model\n",
    "throughput_model = tf.keras.models.load_model(\"/home/research-student/omnet-fanet/nn_checkpoints/throughput_predict_nn_v4_multimodulation_novideo_sinr_dl/model.003-0.0017.h5\", compile=False)\n",
    "throughput_model.compile(optimizer='adam', \n",
    "              loss={'throughput': 'mse'},\n",
    "              metrics={'throughput': 'accuracy'})\n",
    "\n",
    "# Load test dataset\n",
    "# test_data_df = pd.read_csv(\"/media/research-student/One Touch/FANET Datasets/Dataset_NP10000_MultiModulation_Hovering_NoVideo/Test/Multi_Modulation_Test_Cases_1_100000_Uplink.csv\")\n",
    "test_data_df = pd.read_csv(\"/media/research-student/One Touch/Conference_Result_Files/Conference_Results/Multi_Modulation_Test_Cases_Downlink.csv\")\n",
    "\n",
    "# Calculate mean and std dev of SINR\n",
    "test_data_df[['Mean_SINR',\"Std_Dev_SINR\"]] = test_data_df.apply(lambda row: sinr_lognormal_approx(row['Horizontal_Distance'],row['Height']),axis=1,result_type='expand')\n",
    "test_data_df[\"Mean_SINR\"] = test_data_df[\"Mean_SINR\"].apply(lambda x: 10*math.log10(x))\n",
    "test_data_df[\"Std_Dev_SINR\"] = test_data_df[\"Std_Dev_SINR\"].apply(lambda x: 10*math.log10(x))\n",
    "\n",
    "mean_sinr = test_data_df[\"Mean_SINR\"].values\n",
    "std_dev_sinr = test_data_df[\"Std_Dev_SINR\"].values\n",
    "\n",
    "# Normalize inputs\n",
    "max_mean_sinr = 10*math.log10(1123) # The max mean SINR calculated at (0,60) is 1122.743643457063 (linear)\n",
    "max_std_dev_sinr = 10*math.log10(466) # The max std dev SINR calculated at (0,60) is 465.2159856885714 (linear)\n",
    "min_mean_sinr = 10*math.log10(0.2) # The min mean SINR calculated at (1200,60) is 0.2251212887895188 (linear)\n",
    "min_std_dev_sinr = 10*math.log10(0.7) # The min std dev SINR calculated at (1200,300) is 0.7160093126585219 (linear)\n",
    "\n",
    "norm_mean_sinr = [2*(m-min_mean_sinr) / (max_mean_sinr-min_mean_sinr) - 1 for m in mean_sinr]\n",
    "norm_std_dev_sinr = [2*(s-min_std_dev_sinr) / (max_std_dev_sinr-min_std_dev_sinr) - 1 for s in std_dev_sinr]\n",
    "norm_uav_send_int = test_data_df[\"UAV_Sending_Interval\"].replace({10:-1, 20:-0.5, 40:0, 100:0.5, 1000:1}).values\n",
    "norm_modulation = test_data_df[\"Modulation\"].replace({\"BPSK\":1, \"QPSK\":0.3333, \"QAM16\":-0.3333, \"QAM64\":-1}).values\n",
    "\n",
    "# For storing prediction results\n",
    "predicted_throughput = []\n",
    "predicted_incr_rcvd = [] \n",
    "predicted_delay_excd = []\n",
    "predicted_queue_overflow = []\n",
    "\n",
    "# Run inference\n",
    "model_inputs = list(zip(norm_mean_sinr, norm_std_dev_sinr, norm_uav_send_int, norm_modulation))\n",
    "prediction = throughput_model.predict(model_inputs)\n",
    "# print(prediction)\n",
    "\n",
    "# Convert output to throughput in bps\n",
    "link_type = 'downlink'\n",
    "if link_type == \"uplink\":\n",
    "    # max_throughput = 10*math.log10(500000) \n",
    "    # min_throughput = 10*math.log10(1) # We make the min throughput to be 1 so that log(min_throughput) does not go to -inf\n",
    "    max_throughput = 500000\n",
    "    min_throughput = 0\n",
    "elif link_type == \"downlink\":\n",
    "    # max_throughput = 10*math.log10(20000) \n",
    "    # min_throughput = 10*math.log10(1) # We make the min throughput to be 1 so that log(min_throughput) does not go to -inf\n",
    "    max_throughput = 20000\n",
    "    min_throughput = 0\n",
    "elif link_type == \"video\":\n",
    "    # max_throughput = 10*math.log10(250000) \n",
    "    # min_throughput = 10*math.log10(1) # We make the min throughput to be 1 so that log(min_throughput) does not go to -inf\n",
    "    max_throughput = 250000 \n",
    "    min_throughput = 0\n",
    "# (10*math.log10(x)-min_throughput)/(max_throughput-min_throughput)\n",
    "# test_data_df['Predicted_Throughput'] = [10**((prob[0]*(max_throughput-min_throughput)+min_throughput)/10) for prob in prediction]\n",
    "test_data_df['Predicted_Throughput'] = [prob[0]*(max_throughput-min_throughput)+min_throughput for prob in prediction]\n",
    "\n",
    "throughput_mae = np.mean(abs(test_data_df['Throughput'].values - test_data_df['Predicted_Throughput'].values))\n",
    "throughput_max_ae = np.max(abs(test_data_df['Throughput'].values - test_data_df['Predicted_Throughput'].values))\n",
    "\n",
    "# Print results\n",
    "print(\"Throughput - MAE: {}, MaxAE: {}\".format(throughput_mae, throughput_max_ae))\n",
    "\n",
    "# test_data_df.to_csv(\"/media/research-student/One Touch/FANET Datasets/Dataset_NP10000_MultiModulation_Hovering_NoVideo/Test/Multi_Modulation_Test_Cases_Uplink_NN_Throughput_RESULTS.csv\")\n",
    "test_data_df.to_csv(\"/media/research-student/One Touch/FANET Datasets/Multi_Modulation_Test_Cases_Downlink_Throughput_NN_RESULTS.csv\")\n",
    "# test_data_df.to_csv(\"/media/research-student/One Touch/FANET Datasets/tmp.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
