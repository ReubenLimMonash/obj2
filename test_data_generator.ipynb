{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ref: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, dataset_details_df, test_split=0.2, type='train', batch_size=32, input_dim=4, n_classes=4, shuffle=None):\n",
    "        'Initialization'\n",
    "        assert ((type == \"train\") or (type == \"Train\") or (type == \"test\") or (type == \"Test\")), \"Type needs to be either 'train'/'test'\"\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_details_df = dataset_details_df\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.type = type\n",
    "        self.dataset_len = self.dataset_details_df[\"Num_Sent\"].sum()\n",
    "        self.cumulative_index = self.dataset_details_df[\"Num_Sent\"].cumsum(axis=0).values # To help with finding the df row based on packet index\n",
    "        self.indexes = np.array([]) # Initialize empty array, make sure this is done before calling self.on_epoch_end()\n",
    "        if self.type == 'train':\n",
    "            self.test_split = test_split\n",
    "        elif self.type == 'test':\n",
    "            self.test_split = 1 - test_split\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        if self.indexes.size > 0:\n",
    "            return int(np.ceil(len(self.indexes) / self.batch_size))\n",
    "        else:\n",
    "            if ((self.type == \"train\") or (self.type == \"Train\")):\n",
    "                return int(np.ceil(self.dataset_len * (1 - self.test_split) / self.batch_size))\n",
    "            elif ((self.type == \"test\") or (self.type == \"Test\")):\n",
    "                return int(np.ceil(self.dataset_len * self.test_split / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == 'all':\n",
    "            if self.test_split == 0 or self.test_split == 1: # Don't split the dataset\n",
    "                self.indexes = np.arange(self.dataset_len)\n",
    "                np.random.shuffle(self.indexes)\n",
    "            else:\n",
    "                self.indexes, _ = train_test_split(np.arange(self.dataset_len), test_size=self.test_split, random_state=0, shuffle=True)\n",
    "            \n",
    "        elif self.shuffle == 'row':\n",
    "            # Shuffle samples within each dataset row only\n",
    "            tmp_indexes = np.arange(self.cumulative_index[0])\n",
    "            if self.test_split == 0 or self.test_split == 1: # Don't split the dataset\n",
    "                np.random.shuffle(tmp_indexes)\n",
    "            else:\n",
    "                tmp_indexes, _ = train_test_split(tmp_indexes, test_size=self.test_split, random_state=0, shuffle=True)\n",
    "            self.indexes = tmp_indexes\n",
    "            for i in range(1, len(self.cumulative_index)):\n",
    "                tmp_indexes = np.arange(self.cumulative_index[i-1], self.cumulative_index[i])\n",
    "                if self.test_split == 0 or self.test_split == 1: # Don't split the dataset\n",
    "                    np.random.shuffle(tmp_indexes)\n",
    "                else:\n",
    "                    tmp_indexes, _ = train_test_split(tmp_indexes, test_size=self.test_split, random_state=0, shuffle=True)\n",
    "                self.indexes = np.append(self.indexes, tmp_indexes)\n",
    "        else:\n",
    "            # Defaults to no shuffle\n",
    "            if self.test_split == 0 or self.test_split == 1: # Don't split the dataset\n",
    "                self.indexes = np.arange(self.dataset_len)\n",
    "            else:\n",
    "                self.indexes, _ = train_test_split(np.arange(self.dataset_len), test_size=self.test_split, random_state=0, shuffle=False)\n",
    "                print(self.test_split)\n",
    "                print(self.dataset_len)\n",
    "                print(len(self.indexes))\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((len(indexes), self.input_dim))\n",
    "        y = np.empty((len(indexes)), dtype=int)\n",
    "        # Generate data\n",
    "        for i, index in enumerate(indexes):\n",
    "            # NOTE: Index in indexes start at 0, bear this is mind when comparing index positions\n",
    "            row_index = np.searchsorted(self.cumulative_index, index, side='right') # The df row this packet index points to (specifying the scenario)\n",
    "            # Make sure to sort the columns in df_row based on the categorical order encoding for packet state: {\"Reliable\":0, \"QUEUE_OVERFLOW\":1, \"RETRY_LIMIT_REACHED\":2, \"Delay_Exceeded\":3}\n",
    "            df_row = self.dataset_details_df.loc[row_index, [\"Mean_SINR\", \"Std_Dev_SINR\", \"UAV_Sending_Interval\", \"Modulation\", \n",
    "                                                             \"Num_Reliable\", \"Num_Q_Overflow\", \"Num_Incr_Rcvd\", \"Num_Delay_Excd\"]].values\n",
    "            if row_index == 0:\n",
    "                packet_state_index = index\n",
    "            else:\n",
    "                packet_state_index = index - self.cumulative_index[row_index-1]\n",
    "\n",
    "            if packet_state_index < df_row[4]:\n",
    "                # Case of reliable packet\n",
    "                packet_state = 0\n",
    "            elif packet_state_index < df_row[4] + df_row[5]:\n",
    "                # Case of queue overflow packet\n",
    "                packet_state = 1\n",
    "            elif packet_state_index < df_row[4] + df_row[5] + df_row[6]:\n",
    "                # Case of incr rcvd packet\n",
    "                packet_state = 2\n",
    "            else:\n",
    "                # Case of delay excd packet\n",
    "                packet_state = 3\n",
    "\n",
    "            # Store sample\n",
    "            X[i,] = df_row[0:4]\n",
    "            # Store class\n",
    "            y[i] = packet_state\n",
    "        \n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "def normalize_data(df, columns=[], save_details_path=None):\n",
    "    '''\n",
    "    columns: The pandas data columns to normalize, given as a list of column names\n",
    "    '''\n",
    "    # Define the ranges of parametrers\n",
    "    max_mean_sinr = 10*math.log10(1123) # The max mean SINR calculated at (0,60) is 1122.743643457063 (linear)\n",
    "    max_std_dev_sinr = 10*math.log10(466) # The max std dev SINR calculated at (0,60) is 465.2159856885714 (linear)\n",
    "    min_mean_sinr = 10*math.log10(0.2) # The min mean SINR calculated at (1200,60) is 0.2251212887895188 (linear)\n",
    "    min_std_dev_sinr = 10*math.log10(0.7) # The min std dev SINR calculated at (1200,300) is 0.7160093126585219 (linear)\n",
    "    max_height = 300\n",
    "    min_height = 60\n",
    "    max_h_dist = 1200\n",
    "    min_h_dist = 0\n",
    "\n",
    "    # Normalize data (Min Max Normalization between [-1,1])\n",
    "    if \"Height\" in columns:\n",
    "        df[\"Height\"] = df[\"Height\"].apply(lambda x: 2*(x-min_height)/(max_height-min_height) - 1)\n",
    "    if \"U2G_H_Dist\" in columns:\n",
    "        df[\"U2G_H_Dist\"] = df[\"U2G_H_Dist\"].apply(lambda x: 2*(x-min_h_dist)/(max_h_dist-min_h_dist) - 1)\n",
    "    if \"Mean_SINR\" in columns:\n",
    "        df[\"Mean_SINR\"] = df[\"Mean_SINR\"].apply(lambda x: 2*(10*math.log10(x)-min_mean_sinr)/(max_mean_sinr-min_mean_sinr) - 1) # Convert to dB space\n",
    "    if \"Std_Dev_SINR\" in columns:\n",
    "        df[\"Std_Dev_SINR\"] = df[\"Std_Dev_SINR\"].apply(lambda x: 2*(10*math.log10(x)-min_std_dev_sinr)/(max_std_dev_sinr-min_std_dev_sinr) - 1) # Convert to dB space\n",
    "    if \"UAV_Sending_Interval\" in columns:\n",
    "        df[\"UAV_Sending_Interval\"] = df[\"UAV_Sending_Interval\"].replace({10:-1, 20:-0.5, 40:0, 66.7: 0.5, 100:1, 1000:2})\n",
    "    if \"Packet_State\" in columns:\n",
    "        df['Packet_State'] = df['Packet_State'].replace({\"Reliable\":0, \"QUEUE_OVERFLOW\":1, \"RETRY_LIMIT_REACHED\":2, \"Delay_Exceeded\":3})\n",
    "    if \"Modulation\" in columns:\n",
    "        df['Modulation'] = df['Modulation'].replace({\"BPSK\":1, \"QPSK\":0.3333, 16:-0.3333, \"QAM-16\":-0.3333, \"QAM16\":-0.3333, 64:-1, \"QAM-64\":-1, \"QAM64\":-1})\n",
    "\n",
    "    # Record details of inputs and output for model\n",
    "    if save_details_path is not None:\n",
    "        f = open(os.path.join(save_details_path,\"model_details.txt\"), \"w\")\n",
    "        f.write(\"Max Height (m): {}\\n\".format(max_height))\n",
    "        f.write(\"Min Height (m): {}\\n\".format(min_height))\n",
    "        f.write(\"Max H_Dist (m): {}\\n\".format(max_h_dist))\n",
    "        f.write(\"Min H_Dist (m): {}\\n\".format(min_h_dist))\n",
    "        f.write(\"Max Mean SINR (dB): {}\\n\".format(max_mean_sinr))\n",
    "        f.write(\"Min Mean SINR (dB): {}\\n\".format(min_mean_sinr))\n",
    "        f.write(\"Max Std Dev SINR (dB): {}\\n\".format(max_std_dev_sinr))\n",
    "        f.write(\"Min Std Dev SINR (dB): {}\\n\".format(min_std_dev_sinr))\n",
    "        f.write(\"[BPSK: 1, QPSK: 0.3333, QAM16: -0.3333, QAM64: -1]\\n\")\n",
    "        f.write(\"UAV Sending Interval: [10:-1, 20:-0.5, 40:0, 100:0.5, 1000:1]\\n\")\n",
    "        f.write(\"Output: ['Reliable':0, 'QUEUE_OVERFLOW':1, 'RETRY_LIMIT_REACHED':2, 'Delay_Exceeded':3]\\n\")\n",
    "        f.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_path = \"/home/research-student/omnetpp_sim_results/PCAP_Test/ParrotAR2_ConstantSI/test.csv\"\n",
    "df_dtypes = {\"Horizontal_Distance\": np.float64, \"Height\": np.int16,\t\"U2G_Distance\": np.int32, \"UAV_Sending_Interval\": np.float64, \"Mean_SINR\": np.float64, \"Std_Dev_SINR\": np.float64,\n",
    "                 \"Num_Sent\": np.int32, \"Num_Reliable\": np.int32, \"Num_Delay_Excd\": np.int32, \"Num_Incr_Rcvd\": np.int32, \"Num_Q_Overflow\": np.int32}\n",
    "dataset_details = pd.read_csv(csv_path, \n",
    "                            usecols = [\"Mean_SINR\", \"Std_Dev_SINR\", \"UAV_Sending_Interval\", \"Modulation\", \"Num_Sent\", \"Num_Reliable\", \"Num_Delay_Excd\",\n",
    "                                        \"Num_Incr_Rcvd\", \"Num_Q_Overflow\"],\n",
    "                            dtype=df_dtypes)\n",
    "\n",
    "dataset_details = normalize_data(dataset_details, columns=[\"Mean_SINR\", \"Std_Dev_SINR\", \"UAV_Sending_Interval\", \"Modulation\"], save_details_path=None)                         \n",
    "data_generator = DataGenerator(dataset_details, test_split=0, type='train', batch_size=100, shuffle='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2122 [00:00<01:23, 25.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2122/2122 [01:20<00:00, 26.24it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = data_generator.__getitem__(0)\n",
    "for i in tqdm(range(1, data_generator.__len__())):\n",
    "    X_i, y_i = data_generator.__getitem__(i)\n",
    "    X = np.append(X, X_i).reshape(-1,4)\n",
    "    y = np.append(y, y_i).reshape(-1,4)\n",
    "\n",
    "df = pd.DataFrame(np.hstack((X,y)), columns=[\"Mean_SINR\", \"Std_Dev_SINR\", \"UAV_Sending_Interval\", \"Modulation\", \"Num_Reliable\", \"Num_Q_Overflow\", \"Num_Incr_Rcvd\", \"Num_Delay_Excd\"])\n",
    "df_recon = []\n",
    "for name, group in df.groupby([\"Mean_SINR\", \"Std_Dev_SINR\"]):\n",
    "    mean_sinr = group[\"Mean_SINR\"].values[0]\n",
    "    std_dev_sinr = group[\"Std_Dev_SINR\"].values[0]\n",
    "    uav_send_int = group[\"UAV_Sending_Interval\"].values[0]\n",
    "    modulation = group[\"Modulation\"].values[0]\n",
    "    num_reliable = group[\"Num_Reliable\"].sum()\n",
    "    num_delay_excd = group[\"Num_Delay_Excd\"].sum()\n",
    "    num_incr_rcvd = group[\"Num_Incr_Rcvd\"].sum()\n",
    "    num_q_overflow = group[\"Num_Q_Overflow\"].sum()\n",
    "    num_sent = num_reliable + num_delay_excd + num_incr_rcvd + num_q_overflow\n",
    "    df_recon.append({\"Mean_SINR\": mean_sinr, \"Std_Dev_SINR\": std_dev_sinr, \"UAV_Sending_Interval\": uav_send_int, \"Modulation\": modulation, \n",
    "                     \"Num_Sent\": num_sent, \"Num_Reliable\": num_reliable, \"Num_Delay_Excd\": num_delay_excd, \"Num_Incr_Rcvd\": num_incr_rcvd, \"Num_Q_Overflow\": num_q_overflow})\n",
    "df_recon = pd.DataFrame(df_recon)\n",
    "df_recon.to_csv(\"/home/research-student/omnetpp_sim_results/PCAP_Test/ParrotAR2_ConstantSI/test_recon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/research-student/omnetpp_sim_results/PCAP_Test/ParrotAR2_ConstantSI/BPSK_Test.csv\")\n",
    "df_recon = []\n",
    "for name, group in df.groupby([\"Mean_SINR\", \"Std_Dev_SINR\"]):\n",
    "    mean_sinr = group[\"Mean_SINR\"].values[0]\n",
    "    std_dev_sinr = group[\"Std_Dev_SINR\"].values[0]\n",
    "    uav_send_int = group[\"UAV_Sending_Interval\"].values[0]\n",
    "    modulation = group[\"Modulation\"].values[0]\n",
    "    num_reliable = group[\"Reliable\"].sum()\n",
    "    num_delay_excd = group[\"Delay_Excd\"].sum()\n",
    "    num_incr_rcvd = group[\"Incr_Rcvd\"].sum()\n",
    "    num_q_overflow = group[\"Q_Overflow\"].sum()\n",
    "    num_sent = num_reliable + num_delay_excd + num_incr_rcvd + num_q_overflow\n",
    "    df_recon.append({\"Mean_SINR\": mean_sinr, \"Std_Dev_SINR\": std_dev_sinr, \"UAV_Sending_Interval\": uav_send_int, \"Modulation\": modulation, \n",
    "                     \"Num_Sent\": num_sent, \"Num_Reliable\": num_reliable, \"Num_Delay_Excd\": num_delay_excd, \"Num_Incr_Rcvd\": num_incr_rcvd, \"Num_Q_Overflow\": num_q_overflow})\n",
    "df_recon = pd.DataFrame(df_recon)\n",
    "df_recon.to_csv(\"/home/research-student/omnetpp_sim_results/PCAP_Test/ParrotAR2_ConstantSI/BPSK_Generator_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/research-student/omnet-fanet/data-processing-scripts/test_data_generator.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bmmen4-0049-re/home/research-student/omnet-fanet/data-processing-scripts/test_data_generator.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m/home/research-student/omnetpp_sim_results/PCAP_Test/ParrotAR2_ConstantSI/test_recon_training.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m         nrows\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:889\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1034\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1088\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1163\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/tf_venv/lib/python3.8/site-packages/pandas/core/dtypes/common.py:1335\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1331\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1332\u001b[0m     )\n\u001b[0;32m-> 1335\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m   1336\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/research-student/omnetpp_sim_results/PCAP_Test/ParrotAR2_ConstantSI/test_recon_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22793873"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
